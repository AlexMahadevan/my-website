---
title: "Wyoming reporter uncovers competitor using AI-generated quotes"
date: "November 8, 2024"
---

![Media Literacy Campaign](./test.jpg "Promoting Media Literacy")

Powell Tribune staff reporter CJ Baker thought a quote from a game warden in another news outlet seemed fishy. Turns out, it was probably generated by artificial intelligence.

Aaron Pelczar resigned from the Cody Enterprise this month after allegedly using generative AI — like ChatGPT — to make up quotes from a liquor store owner, an astronomer and a deputy district attorney, among others.

Baker began calling sources to verify quotes after reading an article about illegal elk hunting. The consensus from these calls: The quotes sounded innocuous and plausible — but they were never spoken. That’s a common characteristic of generative AI text.

Baker met with Pelczar and his editor in person with evidence about at least seven people who were quoted, but never interviewed. “It’s never comfortable to confront someone, but it’s especially uncomfortable when it involves colleagues (and competitors) in the media world,” Baker said. “What helped is that the editor at the Cody Enterprise, Chris Bacon, was gracious and receptive.”

In another oddity that’s characteristic of generative AI, the suspected stories added incorrect roles and educational titles to sources.

In one story, a professor of extragalactic astronomy was referred to as a meteorologist, and the “fireball report coordinator” for the American Meteor Society was given the title of doctor of astronomy.

And in one article, the author — likely AI — “‘took some liberties’ with what researchers know about raccoons’ cognition versus other species,” according to Baker’s reporting.

The Cody Enterprise, which was founded by William Frederick Cody, better known as Buffalo Bill, put out a statement that framed the incident as an “advanced form of plagiarism,” and said, “We now have a system set in place to catch AI generated stories. We will have longer conversations about how AI-generated stories are not acceptable, we will hold our employees to a higher standard and we stand by that.”

The outlet attached corrections to four articles, ranging from a story about a rare meteor shower to another about an embezzlement scandal.

“This sort of deception by a reporter is very similar to the old-fashioned ethical failures of plagiarism and fabrication. It’s what Jayson Blair did at The New York Times more than 20 years ago,” said Kelly McBride, Poynter’s senior vice president and the chair of the Craig Newmark Center for Ethics and Leadership. “He got caught when a reporter at a smaller paper called him out for plagiarizing her work.”

McBride said newsrooms can learn from this type of situation: There has to be close collaboration between reporters and editors. And that might include asking for a reporter’s notes if something seems off.

“Editors who brainstorm out with a reporter the purpose and the tone of a story ahead of time, as well as the reporting strategy, will have a lot of follow-up questions when some weird AI crap gets turned in,” she said.

The Cody Enterprise doesn’t have a public AI ethics policy. And this incident illustrates a weakness in many news organizations’ AI policies: They lack teeth. In this case, Pelczar resigned, and the paper issued its column and added corrections, but readers should always know what the consequences are for violating the trust of ethical AI use.

“It’s unfortunate, because we already know that the public thinks generative AI can be a form of journalistic cheating. And in this case, it was,” McBride said. “But there are going to be valid applications of AI in journalism. If the public doesn’t trust any of it, even when the application is good, they won’t believe the information.”